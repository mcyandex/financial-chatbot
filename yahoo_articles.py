# -*- coding: utf-8 -*-
"""TP4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/109KZb14LaA9dgOLDwa-sSSVOuV1H1I6j

*TP4* : Création d'un Chatbot basé sur le Machine Learning  
Introduction : Dans ce TP, vous allez créer un chatbot qui utilise un modèle de traitement du langage naturel
(NLP) basé sur l'apprentissage automatique pour répondre à des questions spécifiques. Suivez les étapes cidessous pour construire votre propre chatbot.
1. Choix de la Base de Données :
Choisissez une base de données ou un ensemble de données qui sera la source d'information pour votre
chatbot. Il peut s'agir de données sur un sujet spécifique, comme des faits généraux, des informations sur
un domaine particulier, etc
"""

import re
import requests
from bs4 import BeautifulSoup
from lxml.html import fromstring
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import random
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import accelerate
import transformers

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def get_yahoo_finance_articles(count=26):
    base_url = "https://finance.yahoo.com/topic/personal-finance-news/"
    response = requests.get(base_url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('li', class_='js-stream-content')

        result = []
        for i, article in enumerate(articles[:count]):
            title = article.find('h3').get_text(strip=True)
            link = article.find('a')['href']
            result.append({"title": title, "link": link})

        return result
    else:
        return None

def print_articles(articles):
  if articles:
    for i, article in enumerate(articles):
        print(f"\nArticle {i + 1}:")
        print(f"Titre: {article['title']}")
        print(f"Liens: {article['link']}")
  else:
      print("Aucun article trouvé.")

def get_titles(articles):
  titles = []
  for i, article in enumerate(articles):
    titles.append(article['title'])
  return titles

def get_links(articles):
  links = []
  for i, article in enumerate(articles):
    if not article['link'].startswith('http'):
      article['link'] = 'https://finance.yahoo.com' + article['link']
    links.append(article['link'])
  return links

articles = get_yahoo_finance_articles()

urls = get_links(articles)

def get_paragraphs_text(soup):
   paragraphs = soup.find_all('p')
   return [re.sub(r'[^a-zA-Z]', ' ', paragraph.text.lower()) for paragraph in paragraphs]

def extract_text_from_article(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'html.parser')
    text = get_paragraphs_text(soup)
    return text

def parse_all_articles(links):
  return [' '.join(extract_text_from_article(link)) for link in links]

bdd = parse_all_articles(urls)

bdd

len(bdd)

"""2. Prétraitement des Données :  
a. Effectuez la tokenization sur votre base de données.  
b. Appliquez la lemmatisation pour normaliser les mots.  
c. Explorez quelques exemples de données prétraitées.
"""

def data_preprocessing(bdd):
  preprocessed_bdd = []
  for doc in bdd:
    tokens = word_tokenize(doc)
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    preprocessed_bdd.append(' '.join(lemmatized_tokens))
  return preprocessed_bdd

preprocessed_bdd = data_preprocessing(bdd)
preprocessed_bdd

"""3. Représentation Vectorielle :  
a. Comprenez le concept de TF-IDF (Term Frequency-Inverse Document Frequency).  
b. Utilisez la méthode TF-IDF pour créer une représentation vectorielle de vos données textuelles.  
c. Examinez quelques vecteurs TF-IDF générés.

Concept de TF-IDF (Term Frequency-Inverse Document Frequency) :
TF-IDF est une technique de pondération couramment utilisée en traitement du langage naturel et en recherche d'information. Elle vise à évaluer l'importance d'un terme dans un document relativement à une collection de documents. La pondération est calculée en multipliant la fréquence d'un terme (TF) par l'inverse de sa fréquence dans tous les documents (IDF).

Term Frequency (TF) : Mesure la fréquence d'un terme dans un document particulier. C'est le nombre de fois où un terme apparaît divisé par le nombre total de termes dans le document.

Inverse Document Frequency (IDF) : Mesure l'inverse de la fréquence du terme dans l'ensemble des documents. Cela permet de donner plus de poids aux termes rares et moins de poids aux termes fréquents.

TF-IDF Score : C'est le produit de TF et IDF.
"""

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(preprocessed_bdd)

def generateResponse(user_input):
  # Append the user input to the list of data sentences
  preprocessed_bdd.append(user_input)

  # Calculate the cosine similarity matrix
  cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

  # Find the most similar sentence to the user input
  most_similar_index = cosine_sim[len(preprocessed_bdd)-1].argsort()[-2]

  # If the most similar sentence is not identical to the user input, return that sentence as the bot's response
  if cosine_sim[len(preprocessed_bdd)-1][most_similar_index] != 1:
      return preprocessed_bdd[most_similar_index]
  else:
      return "I am sorry, I could not understand you."

feature_names = vectorizer.get_feature_names_out()
for i, document in enumerate(preprocessed_bdd):
    print(f"Document {i + 1}: {document}")
    for j, feature in enumerate(feature_names):
        tfidf_value = tfidf_matrix[i, j]
        if tfidf_value > 0:
            print(f"  {feature}: {tfidf_value}")
    print("="*40)

for i, document in enumerate(preprocessed_bdd):
    tfidf_values = tfidf_matrix[i, :].toarray()[0]
    top5_indices = np.argsort(tfidf_values)[::-1][:5]
    top5_words = [feature_names[index] for index in top5_indices]
    print(f"Top 5 des mots les plus fréquents dans le document {i + 1}: {top5_words}")

total_tfidf_values = np.sum(tfidf_matrix, axis=0).A1
top5_indices_total = np.argsort(total_tfidf_values)[::-1][:5]
top5_words_total = [feature_names[index] for index in top5_indices_total]
print(f"\nTop 5 des mots les plus fréquents sur l'ensemble des documents : {top5_words_total}")

"""4. Choix du Modèle NLP :  
a. Choisissez un modèle NLP parmi les options populaires comme BERT, GPT, DistilBERT, etc.  
b. Expliquez la raison de votre choix en fonction de votre cas d'utilisation.

Pour des articles financiers, où la compréhension contextuelle est cruciale et où les relations complexes entre les termes peuvent être importantes, BERT serait un choix solide. BERT excelle dans la compréhension contextuelle bidirectionnelle, ce qui est particulièrement utile pour saisir les nuances et le contexte spécifique aux domaines tels que la finance.

Les articles financiers peuvent souvent contenir des termes techniques, des acronymes, et des relations subtiles entre les entités, et BERT est bien adapté pour capturer ces informations. De plus, BERT peut être fine-tuné sur des tâches spécifiques liées à la finance, comme la classification de sentiments, l'extraction d'entités financières, etc.

5. Entraînement du Modèle :  
a. Divisez votre base de données en ensembles d'entraînement et de test.  
b. Entraînez votre modèle NLP sur l'ensemble d'entraînement.  
c. Évaluez les performances de votre modèle sur l'ensemble de test.
"""

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import torch

# Initialisation du tokenizer et du modèle BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Préparation des données pour l'entraînement
inputs = tokenizer(preprocessed_bdd, return_tensors='pt', padding=True, truncation=True)
labels = torch.tensor([0]*len(preprocessed_bdd)) # Remplacez par vos labels si vous en avez

# Division des données en ensembles d'entraînement et de test
train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)

# Création des arguments d'entraînement
training_args = TrainingArguments(
   output_dir='./results',         # Répertoire de sortie pour les résultats de l'entraînement
   num_train_epochs=3,             # Nombre total d'epochs d'entraînement
   per_device_train_batch_size=16, # Taille du lot pour l'entraînement
   per_device_eval_batch_size=64,  # Taille du lot pour l'évaluation
   warmup_steps=500,               # Nombre d'itérations avant l'augmentation de la température
   weight_decay=0.01,              # Taux de décroissance de l'entraînement
   logging_dir='./logs',           # Répertoire pour les logs
)

# Entraînement du modèle
trainer = Trainer(
   model=model,                       # Le modèle à entraîner
   args=training_args,                # Arguments pour l'entraînement
   train_dataset=(train_inputs, train_labels), # Ensemble de données d'entraînement
   eval_dataset=(test_inputs, test_labels)     # Ensemble de données d'évaluation
)

trainer.train()

# Évaluation du modèle
eval_result = trainer.evaluate()
print(eval_result)

kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix)
kmeans.labels_

from torch.utils.data import Dataset

class FinancialArticlesDataset(Dataset):
   def __init__(self, inputs, labels):
       self.inputs = inputs
       self.labels = labels

   def __len__(self):
       return len(self.inputs)

   def __getitem__(self, idx):
       return self.inputs[idx], self.labels[idx]

train_inputs, test_inputs, train_labels, test_labels = train_test_split(tfidf_matrix, kmeans.labels_, test_size=0.2, random_state=42)

# Conversion des listes en objets Dataset
train_dataset = FinancialArticlesDataset(train_inputs, train_labels)
test_dataset = FinancialArticlesDataset(test_inputs, test_labels)

# Entraînement du modèle
trainer = Trainer(
 model=model,                   # Le modèle à entraîner
 args=training_args,              # Arguments pour l'entraînement
 train_dataset=train_dataset,    # Ensemble de données d'entraînement
 eval_dataset=test_dataset        # Ensemble de données d'évaluation
)

trainer.train()

# Évaluation du modèle
eval_result = trainer.evaluate()
print(eval_result)

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

# Division des données en ensembles d'entraînement et de test
train_inputs, test_inputs, train_labels, test_labels = train_test_split(tfidf_matrix, kmeans.labels_, test_size=0.2, random_state=42)

# Création des arguments d'entraînement
training_args = TrainingArguments(
  output_dir='./results',        # Répertoire de sortie pour les résultats de l'entraînement
  num_train_epochs=3,            # Nombre total d'epochs d'entraînement
  per_device_train_batch_size=16, # Taille du lot pour l'entraînement
  per_device_eval_batch_size=64, # Taille du lot pour l'évaluation
  warmup_steps=500,              # Nombre d'itérations avant l'augmentation de la température
  weight_decay=0.01,             # Taux de décroissance de l'entraînement
  logging_dir='./logs',          # Répertoire pour les logs
)

# Entraînement du modèle
trainer = Trainer(
  model=model,                     # Le modèle à entraîner
  args=training_args,               # Arguments pour l'entraînement
  train_dataset=(train_inputs, train_labels), # Ensemble de données d'entraînement
  eval_dataset=(test_inputs, test_labels)    # Ensemble de données d'évaluation
)

trainer.train()

# Évaluation du modèle
eval_result = trainer.evaluate()
print(eval_result)


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
import torch


# Division des données en ensembles d'entraînement et de test
train_texts, test_texts, train_labels, test_labels = train_test_split(preprocessed_bdd, kmeans.labels_, test_size=0.2, random_state=42)
train_texts = [str(text) for text in train_texts]
test_texts = [str(text) for text in test_texts]
# Tokenization et prétraitement
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')
test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt')

# Création des ensembles de données PyTorch
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))

# Modèle BERT pour la classification binaire (par exemple)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Entraînement du modèle
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)


model.train()
for epoch in range(3):  # Nombre d'époques, ajustez selon votre besoin
    for batch in train_dataloader:
        optimizer.zero_grad()
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.long())
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Évaluation sur l'ensemble de test
test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)
model.eval()
all_predictions = []
with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        all_predictions.extend(predictions.cpu().numpy())

# Évaluation des performances
accuracy = accuracy_score(test_labels, all_predictions)
classification_report_str = classification_report(test_labels, all_predictions)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", classification_report_str)

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le modèle pré-entraîné BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 classes pour la classification binaire

# Supposons que vous avez des données avec des textes et des étiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Padding et création de tensors
max_len = max(len(text) for text in tokenized_texts)
input_ids = torch.tensor([text + [0] * (max_len - len(text)) for text in tokenized_texts])
labels = torch.tensor(labels)

# Création du dataset
dataset = TensorDataset(input_ids, labels)

# Division en ensembles d'entraînement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Paramètres d'entraînement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entraînement
for epoch in range(epochs):
    model.train()
    for batch in tqdm(train_dataloader, desc="Training"):
        input_ids, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Évaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
    input_ids, labels = batch
    with torch.no_grad():
        outputs = model(input_ids)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    all_preds.extend(preds.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calcul de la précision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le modèle pré-entraîné BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2) # 2 classes pour la classification binaire

# Supposons que vous avez des données avec des textes et des étiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Troncature des séquences à une longueur maximale de 512 tokens
truncated_texts = [text[:512] for text in tokenized_texts]

# Padding et création de tensors
max_len = max(len(text) for text in truncated_texts)
input_ids = torch.tensor([text + [0] * (max_len - len(text)) for text in truncated_texts])
labels = torch.tensor(labels)

# Création du masque d'attention
attention_mask = torch.zeros(input_ids.shape, dtype=torch.long)
for i, text in enumerate(truncated_texts):
   attention_mask[i, :len(text)] = 1

# Création du dataset
dataset = TensorDataset(input_ids, attention_mask, labels)

# Division en ensembles d'entraînement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Paramètres d'entraînement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entraînement
for epoch in range(epochs):
   model.train()
   for batch in tqdm(train_dataloader, desc="Training"):
       input_ids, attention_mask, labels = batch
       optimizer.zero_grad()
       outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
       loss = outputs.loss
       loss.backward()
       optimizer.step()

# Évaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
   input_ids, attention_mask, labels = batch
   with torch.no_grad():
       outputs = model(input_ids, attention_mask=attention_mask)
   logits = outputs.logits
   preds = torch.argmax(logits, dim=1)
   all_preds.extend(preds.cpu().numpy())
   all_labels.extend(labels.cpu().numpy())

# Calcul de la précision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le modèle pré-entraîné BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 classes pour la classification binaire

# Supposons que vous avez des données avec des textes et des étiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes et gestion des masques d'attention
max_seq_length = 512
tokenized_texts = [tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='pt') for text in texts]

input_ids = torch.cat([text['input_ids'] for text in tokenized_texts], dim=0)
attention_masks = torch.cat([text['attention_mask'] for text in tokenized_texts], dim=0)
labels = torch.tensor(labels, dtype=torch.long)

# Création du dataset
dataset = TensorDataset(input_ids, attention_masks, labels)

# Division en ensembles d'entraînement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Paramètres d'entraînement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entraînement
for epoch in range(epochs):
    model.train()
    for batch in tqdm(train_dataloader, desc="Training"):
        input_ids, attention_masks, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Évaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
    input_ids, attention_masks, labels = batch
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_masks)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    all_preds.extend(preds.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calcul de la précision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

"""6. Interaction avec le Chatbot :  
a. Implémentez une fonction de chatbot qui prend une question en entrée.  
b. Utilisez le modèle NLP pour générer une réponse basée sur la similarité avec les données d'entraînement.
"""

def start_chatbot():

   exit_conditions = (":q", "quit", "exit")

   while True:
       query = input("> ")

       if query in exit_conditions:
           break
       else:
           response = get_response(query) #get_response a implémenté
           print(f"🤖 {response}")

start_chatbot()

"""7. Évaluation du Chatbot :  
a. Posez plusieurs questions au chatbot et évaluez la qualité des réponses.  
b. Proposez des améliorations possibles pour rendre le chatbot plus performant.

Conclusion :   
Résumez les principales étapes du processus de création du chatbot et discutez des défis rencontrés.
"""