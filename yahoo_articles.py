<<<<<<< HEAD
# -*- coding: utf-8 -*-
"""TP4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/109KZb14LaA9dgOLDwa-sSSVOuV1H1I6j

*TP4* : Création d'un Chatbot basé sur le Machine Learning  
Introduction : Dans ce TP, vous allez créer un chatbot qui utilise un modèle de traitement du langage naturel
(NLP) basé sur l'apprentissage automatique pour répondre à des questions spécifiques. Suivez les étapes cidessous pour construire votre propre chatbot.
1. Choix de la Base de Données :
Choisissez une base de données ou un ensemble de données qui sera la source d'information pour votre
chatbot. Il peut s'agir de données sur un sujet spécifique, comme des faits généraux, des informations sur
un domaine particulier, etc
"""

import re
=======
>>>>>>> 3c602b01e0c8e106c15b2df638d1ca4b17d83635
import requests
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def get_yahoo_finance_articles(base_url, count=26):
    """
    Retrieves Yahoo Finance articles from the specified base URL.

    Parameters:
    - base_url (str): The base URL to fetch articles from.
    - count (int): Number of articles to retrieve.

    Returns:
    - list: List of dictionaries containing article titles and links.
    """
    response = requests.get(base_url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('li', class_='js-stream-content')

        result = []
        for i, article in enumerate(articles[:count]):
            title = article.find('h3').get_text(strip=True)
            link = article.find('a')['href']
            result.append({"title": title, "link": link})

        return result
    else:
        return None

def print_articles(articles):
  if articles:
    for i, article in enumerate(articles):
        print(f"\nArticle {i + 1}:")
        print(f"Titre: {article['title']}")
        print(f"Liens: {article['link']}")
  else:
      print("Aucun article trouvé.")

def get_titles(articles):
  titles = []
  for i, article in enumerate(articles):
    titles.append(article['title'])
  return titles

def get_links(articles):
  links = []
  for i, article in enumerate(articles):
    if not article['link'].startswith('http'):
      article['link'] = 'https://finance.yahoo.com' + article['link']
    links.append(article['link'])
  return links

def get_paragraphs_text(soup):
   paragraphs = soup.find_all('p')
   return [paragraph.text.lower() for paragraph in paragraphs]

def extract_text_from_article(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'html.parser')
    text = get_paragraphs_text(soup)
    return text

def parse_all_articles(links):
  return ['.'.join(extract_text_from_article(link)) for link in links]

def data_preprocessing(bdd):
  preprocessed_bdd = []
  for doc in bdd:
    tokens = sent_tokenize(doc)
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    preprocessed_bdd.append(' '.join(lemmatized_tokens))
  return preprocessed_bdd

def get_best_article(user_input):
  """
    Finds the most similar preprocessed article to the user input.

    Parameters:
    - user_input (str): User's input.

    Returns:
    - str: The most similar preprocessed article or an error message.
    """
  preprocessed_bdd.append(user_input)

  vectorizer = TfidfVectorizer(stop_words='english')
  tfidf_matrix = vectorizer.fit_transform(preprocessed_bdd)

  cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])
  most_similar_index = np.argmax(cosine_sim)

  # Check if the cosine similarity is less than 1 (indicating some similarity) and greater than 0.1 (considering a minimum threshold for similarity).
  if cosine_sim[0][most_similar_index] < 1 and cosine_sim[0][most_similar_index] > 0.1:
      return preprocessed_bdd[most_similar_index]
  else:
      return "I am sorry, I could not understand you."
  
def get_following_sentences(user_input, best_article, num_following_sentences=5):
    """
    Finds the N most similar sentences following the best article.

    Parameters:
    - user_input (str): User's input.
    - best_article (str): The most similar preprocessed article.
    - num_following_sentences (int): Number of following sentences to retrieve.

    Returns:
    - str: The capitalized N most similar sentences or an error message.
    """
    best_article = best_article.split('.') 
    best_article.append(user_input)

    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(best_article)

    cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])

    most_similar_index = np.argmax(cosine_sim)

    following_indices = np.argsort(cosine_sim[0])[:-num_following_sentences-1:-1][1:]

    following_sentences = [best_article[i] for i in following_indices]

<<<<<<< HEAD
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import torch

# Initialisation du tokenizer et du modèle BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Préparation des données pour l'entraînement
inputs = tokenizer(preprocessed_bdd, return_tensors='pt', padding=True, truncation=True)
labels = torch.tensor([0]*len(preprocessed_bdd)) # Remplacez par vos labels si vous en avez

# Division des données en ensembles d'entraînement et de test
train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)

# Création des arguments d'entraînement
training_args = TrainingArguments(
   output_dir='./results',         # Répertoire de sortie pour les résultats de l'entraînement
   num_train_epochs=3,             # Nombre total d'epochs d'entraînement
   per_device_train_batch_size=16, # Taille du lot pour l'entraînement
   per_device_eval_batch_size=64,  # Taille du lot pour l'évaluation
   warmup_steps=500,               # Nombre d'itérations avant l'augmentation de la température
   weight_decay=0.01,              # Taux de décroissance de l'entraînement
   logging_dir='./logs',           # Répertoire pour les logs
)

# Entraînement du modèle
trainer = Trainer(
   model=model,                       # Le modèle à entraîner
   args=training_args,                # Arguments pour l'entraînement
   train_dataset=(train_inputs, train_labels), # Ensemble de données d'entraînement
   eval_dataset=(test_inputs, test_labels)     # Ensemble de données d'évaluation
)

trainer.train()

# Évaluation du modèle
eval_result = trainer.evaluate()
print(eval_result)

kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix)
kmeans.labels_

from torch.utils.data import Dataset

class FinancialArticlesDataset(Dataset):
   def __init__(self, inputs, labels):
       self.inputs = inputs
       self.labels = labels

   def __len__(self):
       return len(self.inputs)

   def __getitem__(self, idx):
       return self.inputs[idx], self.labels[idx]

train_inputs, test_inputs, train_labels, test_labels = train_test_split(tfidf_matrix, kmeans.labels_, test_size=0.2, random_state=42)

# Conversion des listes en objets Dataset
train_dataset = FinancialArticlesDataset(train_inputs, train_labels)
test_dataset = FinancialArticlesDataset(test_inputs, test_labels)

# Entraînement du modèle
trainer = Trainer(
 model=model,                   # Le modèle à entraîner
 args=training_args,              # Arguments pour l'entraînement
 train_dataset=train_dataset,    # Ensemble de données d'entraînement
 eval_dataset=test_dataset        # Ensemble de données d'évaluation
)

trainer.train()

# Évaluation du modèle
eval_result = trainer.evaluate()
print(eval_result)

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

# Division des données en ensembles d'entraînement et de test
train_inputs, test_inputs, train_labels, test_labels = train_test_split(tfidf_matrix, kmeans.labels_, test_size=0.2, random_state=42)

# Création des arguments d'entraînement
training_args = TrainingArguments(
  output_dir='./results',        # Répertoire de sortie pour les résultats de l'entraînement
  num_train_epochs=3,            # Nombre total d'epochs d'entraînement
  per_device_train_batch_size=16, # Taille du lot pour l'entraînement
  per_device_eval_batch_size=64, # Taille du lot pour l'évaluation
  warmup_steps=500,              # Nombre d'itérations avant l'augmentation de la température
  weight_decay=0.01,             # Taux de décroissance de l'entraînement
  logging_dir='./logs',          # Répertoire pour les logs
)

# Entraînement du modèle
trainer = Trainer(
  model=model,                     # Le modèle à entraîner
  args=training_args,               # Arguments pour l'entraînement
  train_dataset=(train_inputs, train_labels), # Ensemble de données d'entraînement
  eval_dataset=(test_inputs, test_labels)    # Ensemble de données d'évaluation
)

trainer.train()

# Évaluation du modèle
eval_result = trainer.evaluate()
print(eval_result)


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
import torch


# Division des données en ensembles d'entraînement et de test
train_texts, test_texts, train_labels, test_labels = train_test_split(preprocessed_bdd, kmeans.labels_, test_size=0.2, random_state=42)
train_texts = [str(text) for text in train_texts]
test_texts = [str(text) for text in test_texts]
# Tokenization et prétraitement
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')
test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt')

# Création des ensembles de données PyTorch
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))

# Modèle BERT pour la classification binaire (par exemple)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Entraînement du modèle
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)


model.train()
for epoch in range(3):  # Nombre d'époques, ajustez selon votre besoin
    for batch in train_dataloader:
        optimizer.zero_grad()
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.long())
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Évaluation sur l'ensemble de test
test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)
model.eval()
all_predictions = []
with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        all_predictions.extend(predictions.cpu().numpy())

# Évaluation des performances
accuracy = accuracy_score(test_labels, all_predictions)
classification_report_str = classification_report(test_labels, all_predictions)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", classification_report_str)

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le modèle pré-entraîné BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 classes pour la classification binaire

# Supposons que vous avez des données avec des textes et des étiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Padding et création de tensors
max_len = max(len(text) for text in tokenized_texts)
input_ids = torch.tensor([text + [0] * (max_len - len(text)) for text in tokenized_texts])
labels = torch.tensor(labels)

# Création du dataset
dataset = TensorDataset(input_ids, labels)

# Division en ensembles d'entraînement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Paramètres d'entraînement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entraînement
for epoch in range(epochs):
    model.train()
    for batch in tqdm(train_dataloader, desc="Training"):
        input_ids, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Évaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
    input_ids, labels = batch
    with torch.no_grad():
        outputs = model(input_ids)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    all_preds.extend(preds.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calcul de la précision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le modèle pré-entraîné BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2) # 2 classes pour la classification binaire

# Supposons que vous avez des données avec des textes et des étiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Troncature des séquences à une longueur maximale de 512 tokens
truncated_texts = [text[:512] for text in tokenized_texts]

# Padding et création de tensors
max_len = max(len(text) for text in truncated_texts)
input_ids = torch.tensor([text + [0] * (max_len - len(text)) for text in truncated_texts])
labels = torch.tensor(labels)

# Création du masque d'attention
attention_mask = torch.zeros(input_ids.shape, dtype=torch.long)
for i, text in enumerate(truncated_texts):
   attention_mask[i, :len(text)] = 1

# Création du dataset
dataset = TensorDataset(input_ids, attention_mask, labels)

# Division en ensembles d'entraînement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Paramètres d'entraînement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entraînement
for epoch in range(epochs):
   model.train()
   for batch in tqdm(train_dataloader, desc="Training"):
       input_ids, attention_mask, labels = batch
       optimizer.zero_grad()
       outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
       loss = outputs.loss
       loss.backward()
       optimizer.step()

# Évaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
   input_ids, attention_mask, labels = batch
   with torch.no_grad():
       outputs = model(input_ids, attention_mask=attention_mask)
   logits = outputs.logits
   preds = torch.argmax(logits, dim=1)
   all_preds.extend(preds.cpu().numpy())
   all_labels.extend(labels.cpu().numpy())

# Calcul de la précision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le modèle pré-entraîné BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 classes pour la classification binaire

# Supposons que vous avez des données avec des textes et des étiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes et gestion des masques d'attention
max_seq_length = 512
tokenized_texts = [tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='pt') for text in texts]

input_ids = torch.cat([text['input_ids'] for text in tokenized_texts], dim=0)
attention_masks = torch.cat([text['attention_mask'] for text in tokenized_texts], dim=0)
labels = torch.tensor(labels, dtype=torch.long)

# Création du dataset
dataset = TensorDataset(input_ids, attention_masks, labels)

# Division en ensembles d'entraînement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Paramètres d'entraînement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entraînement
for epoch in range(epochs):
    model.train()
    for batch in tqdm(train_dataloader, desc="Training"):
        input_ids, attention_masks, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Évaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
    input_ids, attention_masks, labels = batch
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_masks)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    all_preds.extend(preds.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calcul de la précision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

"""6. Interaction avec le Chatbot :  
a. Implémentez une fonction de chatbot qui prend une question en entrée.  
b. Utilisez le modèle NLP pour générer une réponse basée sur la similarité avec les données d'entraînement.
"""

def start_chatbot():

   exit_conditions = (":q", "quit", "exit")
=======
    # Check if the cosine similarity is less than 1 (indicating some similarity) and greater than 0.1 (considering a minimum threshold for similarity).
    if cosine_sim[0][most_similar_index] < 1 and cosine_sim[0][most_similar_index] > 0.1:
        capitalized_sentences = [sentence.capitalize() for sentence in following_sentences]
        return '. '.join(capitalized_sentences) + '.'
    else:
        return "I am sorry, I could not understand you."
>>>>>>> 3c602b01e0c8e106c15b2df638d1ca4b17d83635

def start_chatbot_yahoo():
   exit_conditions = ("q", "quit", "exit", 'bye')
   while True:
       query = input("\nUser: ")
       if query in exit_conditions:
           break
       else:
           best_article = get_best_article(query)
           best_sentence = get_following_sentences(query, best_article)
           print(f"\nChatbot: {best_sentence}")
           preprocessed_bdd.pop()

def get_financial_advices():
  articles = get_yahoo_finance_articles("https://finance.yahoo.com/topic/personal-finance-news/") + get_yahoo_finance_articles("https://finance.yahoo.com/") + get_yahoo_finance_articles("https://finance.yahoo.com/calendar/") + get_yahoo_finance_articles("https://finance.yahoo.com/topic/stock-market-news/")
  urls = get_links(articles)
  global bdd
  bdd = parse_all_articles(urls)
  global preprocessed_bdd
  preprocessed_bdd = data_preprocessing(bdd)
  start_chatbot_yahoo()