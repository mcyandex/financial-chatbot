# -*- coding: utf-8 -*-
"""TP4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/109KZb14LaA9dgOLDwa-sSSVOuV1H1I6j

*TP4* : Cr√©ation d'un Chatbot bas√© sur le Machine Learning  
Introduction : Dans ce TP, vous allez cr√©er un chatbot qui utilise un mod√®le de traitement du langage naturel
(NLP) bas√© sur l'apprentissage automatique pour r√©pondre √† des questions sp√©cifiques. Suivez les √©tapes cidessous pour construire votre propre chatbot.
1. Choix de la Base de Donn√©es :
Choisissez une base de donn√©es ou un ensemble de donn√©es qui sera la source d'information pour votre
chatbot. Il peut s'agir de donn√©es sur un sujet sp√©cifique, comme des faits g√©n√©raux, des informations sur
un domaine particulier, etc
"""

import re
import requests
from bs4 import BeautifulSoup
from lxml.html import fromstring
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import random
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import accelerate
import transformers

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def get_yahoo_finance_articles(count=26):
    base_url = "https://finance.yahoo.com/topic/personal-finance-news/"
    response = requests.get(base_url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('li', class_='js-stream-content')

        result = []
        for i, article in enumerate(articles[:count]):
            title = article.find('h3').get_text(strip=True)
            link = article.find('a')['href']
            result.append({"title": title, "link": link})

        return result
    else:
        return None

def print_articles(articles):
  if articles:
    for i, article in enumerate(articles):
        print(f"\nArticle {i + 1}:")
        print(f"Titre: {article['title']}")
        print(f"Liens: {article['link']}")
  else:
      print("Aucun article trouv√©.")

def get_titles(articles):
  titles = []
  for i, article in enumerate(articles):
    titles.append(article['title'])
  return titles

def get_links(articles):
  links = []
  for i, article in enumerate(articles):
    if not article['link'].startswith('http'):
      article['link'] = 'https://finance.yahoo.com' + article['link']
    links.append(article['link'])
  return links

articles = get_yahoo_finance_articles()

urls = get_links(articles)

def get_paragraphs_text(soup):
   paragraphs = soup.find_all('p')
   return [re.sub(r'[^a-zA-Z]', ' ', paragraph.text.lower()) for paragraph in paragraphs]

def extract_text_from_article(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'html.parser')
    text = get_paragraphs_text(soup)
    return text

def parse_all_articles(links):
  return [' '.join(extract_text_from_article(link)) for link in links]

bdd = parse_all_articles(urls)

bdd

len(bdd)

"""2. Pr√©traitement des Donn√©es :  
a. Effectuez la tokenization sur votre base de donn√©es.  
b. Appliquez la lemmatisation pour normaliser les mots.  
c. Explorez quelques exemples de donn√©es pr√©trait√©es.
"""

def data_preprocessing(bdd):
  preprocessed_bdd = []
  for doc in bdd:
    tokens = word_tokenize(doc)
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    preprocessed_bdd.append(' '.join(lemmatized_tokens))
  return preprocessed_bdd

preprocessed_bdd = data_preprocessing(bdd)
preprocessed_bdd

"""3. Repr√©sentation Vectorielle :  
a. Comprenez le concept de TF-IDF (Term Frequency-Inverse Document Frequency).  
b. Utilisez la m√©thode TF-IDF pour cr√©er une repr√©sentation vectorielle de vos donn√©es textuelles.  
c. Examinez quelques vecteurs TF-IDF g√©n√©r√©s.

Concept de TF-IDF (Term Frequency-Inverse Document Frequency) :
TF-IDF est une technique de pond√©ration couramment utilis√©e en traitement du langage naturel et en recherche d'information. Elle vise √† √©valuer l'importance d'un terme dans un document relativement √† une collection de documents. La pond√©ration est calcul√©e en multipliant la fr√©quence d'un terme (TF) par l'inverse de sa fr√©quence dans tous les documents (IDF).

Term Frequency (TF) : Mesure la fr√©quence d'un terme dans un document particulier. C'est le nombre de fois o√π un terme appara√Æt divis√© par le nombre total de termes dans le document.

Inverse Document Frequency (IDF) : Mesure l'inverse de la fr√©quence du terme dans l'ensemble des documents. Cela permet de donner plus de poids aux termes rares et moins de poids aux termes fr√©quents.

TF-IDF Score : C'est le produit de TF et IDF.
"""

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(preprocessed_bdd)

def generateResponse(user_input):
  # Append the user input to the list of data sentences
  preprocessed_bdd.append(user_input)

  # Calculate the cosine similarity matrix
  cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

  # Find the most similar sentence to the user input
  most_similar_index = cosine_sim[len(preprocessed_bdd)-1].argsort()[-2]

  # If the most similar sentence is not identical to the user input, return that sentence as the bot's response
  if cosine_sim[len(preprocessed_bdd)-1][most_similar_index] != 1:
      return preprocessed_bdd[most_similar_index]
  else:
      return "I am sorry, I could not understand you."

feature_names = vectorizer.get_feature_names_out()
for i, document in enumerate(preprocessed_bdd):
    print(f"Document {i + 1}: {document}")
    for j, feature in enumerate(feature_names):
        tfidf_value = tfidf_matrix[i, j]
        if tfidf_value > 0:
            print(f"  {feature}: {tfidf_value}")
    print("="*40)

for i, document in enumerate(preprocessed_bdd):
    tfidf_values = tfidf_matrix[i, :].toarray()[0]
    top5_indices = np.argsort(tfidf_values)[::-1][:5]
    top5_words = [feature_names[index] for index in top5_indices]
    print(f"Top 5 des mots les plus fr√©quents dans le document {i + 1}: {top5_words}")

total_tfidf_values = np.sum(tfidf_matrix, axis=0).A1
top5_indices_total = np.argsort(total_tfidf_values)[::-1][:5]
top5_words_total = [feature_names[index] for index in top5_indices_total]
print(f"\nTop 5 des mots les plus fr√©quents sur l'ensemble des documents : {top5_words_total}")

"""4. Choix du Mod√®le NLP :  
a. Choisissez un mod√®le NLP parmi les options populaires comme BERT, GPT, DistilBERT, etc.  
b. Expliquez la raison de votre choix en fonction de votre cas d'utilisation.

Pour des articles financiers, o√π la compr√©hension contextuelle est cruciale et o√π les relations complexes entre les termes peuvent √™tre importantes, BERT serait un choix solide. BERT excelle dans la compr√©hension contextuelle bidirectionnelle, ce qui est particuli√®rement utile pour saisir les nuances et le contexte sp√©cifique aux domaines tels que la finance.

Les articles financiers peuvent souvent contenir des termes techniques, des acronymes, et des relations subtiles entre les entit√©s, et BERT est bien adapt√© pour capturer ces informations. De plus, BERT peut √™tre fine-tun√© sur des t√¢ches sp√©cifiques li√©es √† la finance, comme la classification de sentiments, l'extraction d'entit√©s financi√®res, etc.

5. Entra√Ænement du Mod√®le :  
a. Divisez votre base de donn√©es en ensembles d'entra√Ænement et de test.  
b. Entra√Ænez votre mod√®le NLP sur l'ensemble d'entra√Ænement.  
c. √âvaluez les performances de votre mod√®le sur l'ensemble de test.
"""

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import torch

# Initialisation du tokenizer et du mod√®le BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Pr√©paration des donn√©es pour l'entra√Ænement
inputs = tokenizer(preprocessed_bdd, return_tensors='pt', padding=True, truncation=True)
labels = torch.tensor([0]*len(preprocessed_bdd)) # Remplacez par vos labels si vous en avez

# Division des donn√©es en ensembles d'entra√Ænement et de test
train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)

# Cr√©ation des arguments d'entra√Ænement
training_args = TrainingArguments(
   output_dir='./results',         # R√©pertoire de sortie pour les r√©sultats de l'entra√Ænement
   num_train_epochs=3,             # Nombre total d'epochs d'entra√Ænement
   per_device_train_batch_size=16, # Taille du lot pour l'entra√Ænement
   per_device_eval_batch_size=64,  # Taille du lot pour l'√©valuation
   warmup_steps=500,               # Nombre d'it√©rations avant l'augmentation de la temp√©rature
   weight_decay=0.01,              # Taux de d√©croissance de l'entra√Ænement
   logging_dir='./logs',           # R√©pertoire pour les logs
)

# Entra√Ænement du mod√®le
trainer = Trainer(
   model=model,                       # Le mod√®le √† entra√Æner
   args=training_args,                # Arguments pour l'entra√Ænement
   train_dataset=(train_inputs, train_labels), # Ensemble de donn√©es d'entra√Ænement
   eval_dataset=(test_inputs, test_labels)     # Ensemble de donn√©es d'√©valuation
)

trainer.train()

# √âvaluation du mod√®le
eval_result = trainer.evaluate()
print(eval_result)

kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix)
kmeans.labels_

from torch.utils.data import Dataset

class FinancialArticlesDataset(Dataset):
   def __init__(self, inputs, labels):
       self.inputs = inputs
       self.labels = labels

   def __len__(self):
       return len(self.inputs)

   def __getitem__(self, idx):
       return self.inputs[idx], self.labels[idx]

train_inputs, test_inputs, train_labels, test_labels = train_test_split(tfidf_matrix, kmeans.labels_, test_size=0.2, random_state=42)

# Conversion des listes en objets Dataset
train_dataset = FinancialArticlesDataset(train_inputs, train_labels)
test_dataset = FinancialArticlesDataset(test_inputs, test_labels)

# Entra√Ænement du mod√®le
trainer = Trainer(
 model=model,                   # Le mod√®le √† entra√Æner
 args=training_args,              # Arguments pour l'entra√Ænement
 train_dataset=train_dataset,    # Ensemble de donn√©es d'entra√Ænement
 eval_dataset=test_dataset        # Ensemble de donn√©es d'√©valuation
)

trainer.train()

# √âvaluation du mod√®le
eval_result = trainer.evaluate()
print(eval_result)

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
train_inputs, test_inputs, train_labels, test_labels = train_test_split(tfidf_matrix, kmeans.labels_, test_size=0.2, random_state=42)

# Cr√©ation des arguments d'entra√Ænement
training_args = TrainingArguments(
  output_dir='./results',        # R√©pertoire de sortie pour les r√©sultats de l'entra√Ænement
  num_train_epochs=3,            # Nombre total d'epochs d'entra√Ænement
  per_device_train_batch_size=16, # Taille du lot pour l'entra√Ænement
  per_device_eval_batch_size=64, # Taille du lot pour l'√©valuation
  warmup_steps=500,              # Nombre d'it√©rations avant l'augmentation de la temp√©rature
  weight_decay=0.01,             # Taux de d√©croissance de l'entra√Ænement
  logging_dir='./logs',          # R√©pertoire pour les logs
)

# Entra√Ænement du mod√®le
trainer = Trainer(
  model=model,                     # Le mod√®le √† entra√Æner
  args=training_args,               # Arguments pour l'entra√Ænement
  train_dataset=(train_inputs, train_labels), # Ensemble de donn√©es d'entra√Ænement
  eval_dataset=(test_inputs, test_labels)    # Ensemble de donn√©es d'√©valuation
)

trainer.train()

# √âvaluation du mod√®le
eval_result = trainer.evaluate()
print(eval_result)


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
import torch


# Division des donn√©es en ensembles d'entra√Ænement et de test
train_texts, test_texts, train_labels, test_labels = train_test_split(preprocessed_bdd, kmeans.labels_, test_size=0.2, random_state=42)
train_texts = [str(text) for text in train_texts]
test_texts = [str(text) for text in test_texts]
# Tokenization et pr√©traitement
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')
test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt')

# Cr√©ation des ensembles de donn√©es PyTorch
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))

# Mod√®le BERT pour la classification binaire (par exemple)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Entra√Ænement du mod√®le
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)


model.train()
for epoch in range(3):  # Nombre d'√©poques, ajustez selon votre besoin
    for batch in train_dataloader:
        optimizer.zero_grad()
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.long())
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# √âvaluation sur l'ensemble de test
test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)
model.eval()
all_predictions = []
with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        all_predictions.extend(predictions.cpu().numpy())

# √âvaluation des performances
accuracy = accuracy_score(test_labels, all_predictions)
classification_report_str = classification_report(test_labels, all_predictions)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", classification_report_str)

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le mod√®le pr√©-entra√Æn√© BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 classes pour la classification binaire

# Supposons que vous avez des donn√©es avec des textes et des √©tiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Padding et cr√©ation de tensors
max_len = max(len(text) for text in tokenized_texts)
input_ids = torch.tensor([text + [0] * (max_len - len(text)) for text in tokenized_texts])
labels = torch.tensor(labels)

# Cr√©ation du dataset
dataset = TensorDataset(input_ids, labels)

# Division en ensembles d'entra√Ænement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Param√®tres d'entra√Ænement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entra√Ænement
for epoch in range(epochs):
    model.train()
    for batch in tqdm(train_dataloader, desc="Training"):
        input_ids, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# √âvaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
    input_ids, labels = batch
    with torch.no_grad():
        outputs = model(input_ids)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    all_preds.extend(preds.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calcul de la pr√©cision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le mod√®le pr√©-entra√Æn√© BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2) # 2 classes pour la classification binaire

# Supposons que vous avez des donn√©es avec des textes et des √©tiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes
tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]

# Troncature des s√©quences √† une longueur maximale de 512 tokens
truncated_texts = [text[:512] for text in tokenized_texts]

# Padding et cr√©ation de tensors
max_len = max(len(text) for text in truncated_texts)
input_ids = torch.tensor([text + [0] * (max_len - len(text)) for text in truncated_texts])
labels = torch.tensor(labels)

# Cr√©ation du masque d'attention
attention_mask = torch.zeros(input_ids.shape, dtype=torch.long)
for i, text in enumerate(truncated_texts):
   attention_mask[i, :len(text)] = 1

# Cr√©ation du dataset
dataset = TensorDataset(input_ids, attention_mask, labels)

# Division en ensembles d'entra√Ænement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Param√®tres d'entra√Ænement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entra√Ænement
for epoch in range(epochs):
   model.train()
   for batch in tqdm(train_dataloader, desc="Training"):
       input_ids, attention_mask, labels = batch
       optimizer.zero_grad()
       outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
       loss = outputs.loss
       loss.backward()
       optimizer.step()

# √âvaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
   input_ids, attention_mask, labels = batch
   with torch.no_grad():
       outputs = model(input_ids, attention_mask=attention_mask)
   logits = outputs.logits
   preds = torch.argmax(logits, dim=1)
   all_preds.extend(preds.cpu().numpy())
   all_labels.extend(labels.cpu().numpy())

# Calcul de la pr√©cision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# Charger le mod√®le pr√©-entra√Æn√© BERT
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 classes pour la classification binaire

# Supposons que vous avez des donn√©es avec des textes et des √©tiquettes (0 ou 1 pour la classification binaire)
texts = preprocessed_bdd
labels = kmeans.labels_

# Tokenization des textes et gestion des masques d'attention
max_seq_length = 512
tokenized_texts = [tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='pt') for text in texts]

input_ids = torch.cat([text['input_ids'] for text in tokenized_texts], dim=0)
attention_masks = torch.cat([text['attention_mask'] for text in tokenized_texts], dim=0)
labels = torch.tensor(labels, dtype=torch.long)

# Cr√©ation du dataset
dataset = TensorDataset(input_ids, attention_masks, labels)

# Division en ensembles d'entra√Ænement et de test
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Param√®tres d'entra√Ænement
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
epochs = 3

# Entra√Ænement
for epoch in range(epochs):
    model.train()
    for batch in tqdm(train_dataloader, desc="Training"):
        input_ids, attention_masks, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# √âvaluation
model.eval()
all_preds = []
all_labels = []

for batch in tqdm(test_dataloader, desc="Testing"):
    input_ids, attention_masks, labels = batch
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_masks)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=1)
    all_preds.extend(preds.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calcul de la pr√©cision
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy: {accuracy}")

"""6. Interaction avec le Chatbot :  
a. Impl√©mentez une fonction de chatbot qui prend une question en entr√©e.  
b. Utilisez le mod√®le NLP pour g√©n√©rer une r√©ponse bas√©e sur la similarit√© avec les donn√©es d'entra√Ænement.
"""

def start_chatbot():

   exit_conditions = (":q", "quit", "exit")

   while True:
       query = input("> ")

       if query in exit_conditions:
           break
       else:
           response = get_response(query) #get_response a impl√©ment√©
           print(f"ü§ñ {response}")

start_chatbot()

"""7. √âvaluation du Chatbot :  
a. Posez plusieurs questions au chatbot et √©valuez la qualit√© des r√©ponses.  
b. Proposez des am√©liorations possibles pour rendre le chatbot plus performant.

Conclusion :   
R√©sumez les principales √©tapes du processus de cr√©ation du chatbot et discutez des d√©fis rencontr√©s.
"""